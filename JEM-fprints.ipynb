{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "MF7BncmmLBeO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "import deepchem as dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPData(Dataset):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSP2qiMqMICK"
   },
   "source": [
    "## Energy-based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "GRYA6JA4LWEC"
   },
   "outputs": [],
   "source": [
    "class EBM(nn.Module):\n",
    "    def __init__(self, energy_net, alpha, sigma, ld_steps, D):\n",
    "        super(EBM, self).__init__()\n",
    "\n",
    "        print('EBM by JT.')\n",
    "\n",
    "        # the neural net used by the EBM\n",
    "        self.energy_net = energy_net\n",
    "\n",
    "        # the loss for classification\n",
    "        self.nll = nn.NLLLoss(reduction='none')  # it requires log-softmax as input!!\n",
    "\n",
    "        # hyperparams\n",
    "        self.D = D\n",
    "\n",
    "        self.sigma = sigma\n",
    "\n",
    "        self.alpha = torch.FloatTensor([alpha])\n",
    "\n",
    "        self.ld_steps = ld_steps\n",
    "\n",
    "    def classify(self, x):\n",
    "        f_xy = self.energy_net(x)\n",
    "        y_pred = torch.softmax(f_xy, 1)\n",
    "        return torch.argmax(y_pred, dim=1)\n",
    "\n",
    "    def class_loss(self, f_xy, y):\n",
    "        # - calculate logits (for classification)\n",
    "        y_pred = torch.softmax(f_xy, 1)\n",
    "\n",
    "        return self.nll(torch.log(y_pred), y)\n",
    "\n",
    "    def gen_loss(self, x, f_xy):\n",
    "        # - sample using Langevine dynamics\n",
    "        x_sample = self.sample(x=None, batch_size=x.shape[0])\n",
    "\n",
    "        # - calculate f(x_sample)[y]\n",
    "        f_x_sample_y = self.energy_net(x_sample)\n",
    "\n",
    "        return -(torch.logsumexp(f_xy, 1) - torch.logsumexp(f_x_sample_y, 1))\n",
    "\n",
    "    def forward(self, x, y, reduction='avg'):\n",
    "        # =====\n",
    "        # forward pass through the network\n",
    "        # - calculate f(x)[y]\n",
    "        f_xy = self.energy_net(x)\n",
    "\n",
    "        # =====\n",
    "        # discriminative part\n",
    "        # - calculate the discriminative loss: the cross-entropy\n",
    "        \n",
    "        y = y.squeeze(1).long()\n",
    "        L_clf = self.class_loss(f_xy, y)\n",
    "\n",
    "        # =====\n",
    "        # generative part\n",
    "        # - calculate the generative loss: E(x) - E(x_sample)\n",
    "        L_gen = self.gen_loss(x, f_xy)\n",
    "\n",
    "        # =====\n",
    "        # Final objective\n",
    "        if reduction == 'sum':\n",
    "            loss = (L_clf + L_gen).sum()\n",
    "        else:\n",
    "            loss = (L_clf + L_gen).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def energy_gradient(self, x):\n",
    "        self.energy_net.eval()\n",
    "\n",
    "        # copy original data that doesn't require grads!\n",
    "        x_i = torch.FloatTensor(x.data)\n",
    "        x_i.requires_grad = True  # WE MUST ADD IT, otherwise autograd won't work\n",
    "\n",
    "        # calculate the gradient\n",
    "        x_i_grad = torch.autograd.grad(torch.logsumexp(self.energy_net(x_i), 1).sum(), [x_i], retain_graph=True)[0]\n",
    "\n",
    "        self.energy_net.train()\n",
    "\n",
    "        return x_i_grad\n",
    "\n",
    "    def langevine_dynamics_step(self, x_old, alpha):\n",
    "        # Calculate gradient wrt x_old\n",
    "        grad_energy = self.energy_gradient(x_old)\n",
    "        # Sample eta ~ Normal(0, alpha)\n",
    "        epsilon = torch.randn_like(grad_energy) * self.sigma\n",
    "\n",
    "        # New sample\n",
    "        x_new = x_old + alpha * grad_energy + epsilon\n",
    "\n",
    "        return x_new\n",
    "\n",
    "    def sample(self, batch_size=64, x=None):\n",
    "        # - 1) Sample from uniform\n",
    "        x_sample = 2. * torch.rand([batch_size, self.D]) - 1.\n",
    "\n",
    "        # - 2) run Langevine Dynamics\n",
    "        for i in range(self.ld_steps):\n",
    "            x_sample = self.langevine_dynamics_step(x_sample, alpha=self.alpha)\n",
    "\n",
    "        return x_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUoPkTmrMVnx"
   },
   "source": [
    "## Evaluation and Training functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvwmRoi7MVto"
   },
   "source": [
    "**Evaluation step, sampling and curve plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "id": "JHx4RIqDLZe9"
   },
   "outputs": [],
   "source": [
    "def evaluation(test_loader, name=None, model_best=None, epoch=None):\n",
    "    # EVALUATION\n",
    "    if model_best is None:\n",
    "        # load best performing model\n",
    "        model_best = torch.load(name + '.model')\n",
    "\n",
    "    model_best.eval()\n",
    "    loss = 0.\n",
    "    loss_error = 0.\n",
    "    loss_gen = 0.\n",
    "    N = 0.\n",
    "    for indx_batch, (test_batch, test_targets) in enumerate(test_loader):\n",
    "        # hybrid loss\n",
    "        loss_t = model_best.forward(test_batch, test_targets, reduction='sum')\n",
    "        loss = loss + loss_t.item()\n",
    "        # classification error\n",
    "        y_pred = model_best.classify(test_batch) #\n",
    "        e = 1.*(y_pred == test_targets)\n",
    "        loss_error = loss_error + (1. - e).sum().item()\n",
    "        # generative nll\n",
    "        f_xy_test = model_best.energy_net(test_batch)\n",
    "        loss_gen = loss_gen + model_best.gen_loss(test_batch, f_xy_test).sum()\n",
    "        # the number of examples\n",
    "        N = N + test_batch.shape[0]\n",
    "    loss = loss / N\n",
    "    loss_error = loss_error / N\n",
    "    loss_gen = loss_gen / N\n",
    "\n",
    "    if epoch is None:\n",
    "        print(f'FINAL PERFORMANCE: nll={loss}, ce={loss_error}, gen_nll={loss_gen}')\n",
    "    else:\n",
    "        print(f'Epoch: {epoch}, val nll={loss}, val ce={loss_error}, val gen_nll={loss_gen}')\n",
    "\n",
    "    return loss, loss_error, loss_gen\n",
    "\n",
    "\n",
    "def samples_real(name, test_loader):\n",
    "    # REAL-------\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    x, _ = next(iter(test_loader))\n",
    "    x = x.detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x[i], (8, 8))\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name+'_real_images.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def samples_generated(name, data_loader, extra_name=''):\n",
    "    # GENERATIONS-------\n",
    "    model_best = torch.load(name + '.model')\n",
    "    model_best.eval()\n",
    "\n",
    "    num_x = 4\n",
    "    num_y = 4\n",
    "    x = model_best.sample(num_x * num_y)\n",
    "    x = x.detach().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(num_x, num_y)\n",
    "    for i, ax in enumerate(ax.flatten()):\n",
    "        plottable_image = np.reshape(x[i], (8, 8))\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.savefig(name + '_generated_images' + extra_name + '.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_curve(name, nll_val, file_name='_nll_val_curve.pdf', color='b-'):\n",
    "    plt.plot(np.arange(len(nll_val)), nll_val, color, linewidth='3')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('nll')\n",
    "    plt.savefig(name + file_name, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def final_eval(test_dataset, name=None, model_best=None, epoch=None):\n",
    "    # EVALUATION\n",
    "    if model_best is None:\n",
    "        # load best performing model\n",
    "        model_best = torch.load(name + '.model')\n",
    "\n",
    "    model_best.eval()\n",
    "    loss = 0.\n",
    "    loss_error = 0.\n",
    "    loss_gen = 0.\n",
    "    N = 0.\n",
    "\n",
    "    y_pred = model_best.classify(test_dataset.X) #\n",
    "    out = roc_auc_score(test_dataset.y, y_pred)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umU3VYKzMbDt"
   },
   "source": [
    "**Training step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "NxkUZ1xVLbm_"
   },
   "outputs": [],
   "source": [
    "def training(name, max_patience, num_epochs, model, optimizer, training_loader, val_loader):\n",
    "    nll_val = []\n",
    "    gen_val = []\n",
    "    error_val = []\n",
    "    best_nll = 1000.\n",
    "    patience = 0\n",
    "\n",
    "    # Main loop\n",
    "    for e in range(num_epochs):\n",
    "        # TRAINING\n",
    "        model.train()\n",
    "        for indx_batch, (batch, targets) in enumerate(training_loader):\n",
    "\n",
    "            loss = model.forward(batch, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        loss_e, error_e, gen_e = evaluation(val_loader, model_best=model, epoch=e)\n",
    "        nll_val.append(loss_e)  # save for plotting\n",
    "        gen_val.append(gen_e.detach().numpy())  # save for plotting\n",
    "        error_val.append(error_e)  # save for plotting\n",
    "\n",
    "        if e == 0:\n",
    "            print('saved!')\n",
    "            torch.save(model, name + '.model')\n",
    "            best_nll = loss_e\n",
    "        else:\n",
    "            if loss_e < best_nll:\n",
    "                print('saved!')\n",
    "                torch.save(model, name + '.model')\n",
    "                best_nll = loss_e\n",
    "                patience = 0\n",
    "\n",
    "                #samples_generated(name, val_loader, extra_name=\"_epoch_\" + str(e))\n",
    "            else:\n",
    "                patience = patience + 1\n",
    "\n",
    "        if patience > max_patience:\n",
    "            break\n",
    "\n",
    "    nll_val = np.asarray(nll_val)\n",
    "    error_val = np.asarray(error_val)\n",
    "    gen_val = np.asarray(gen_val)\n",
    "\n",
    "    return nll_val, error_val, gen_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BXJ9dN0MinB"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsF7f-Q-MkWu"
   },
   "source": [
    "**Initialize datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "fqZKMNM0LdQ1"
   },
   "outputs": [],
   "source": [
    "tasks, datasets, transformers = dc.molnet.load_bace_classification(featurizer = 'ECFP')\n",
    "train_data, valid_data, test_data = datasets\n",
    "\n",
    "X = torch.tensor(train_data.X).float()\n",
    "y = torch.tensor(train_data.y).float()\n",
    "train_dataset = TensorDataset(X, y)\n",
    "X = torch.tensor(test_data.X).float()\n",
    "y = torch.tensor(test_data.y).float()\n",
    "test_dataset = TensorDataset(X, y)\n",
    "X = torch.tensor(valid_data.X).float()\n",
    "y = torch.tensor(valid_data.y).float()\n",
    "valid_dataset = TensorDataset(X, y)\n",
    "\n",
    "training_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True, drop_last = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 16, shuffle = True, drop_last = True)\n",
    "val_loader = DataLoader(valid_dataset, batch_size = 16, shuffle = True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lEKUznpMns7"
   },
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "ANQo7LrGLjIN"
   },
   "outputs": [],
   "source": [
    "D = 1024  # input dimension\n",
    "K = 2 # output dimension\n",
    "M = 16  # the number of neurons\n",
    "\n",
    "sigma = 0.01 # the noise level\n",
    "\n",
    "alpha = 1.  # the step-size for SGLD\n",
    "ld_steps = 20  # the number of steps of SGLD\n",
    "\n",
    "lr = 1e-3  # learning rate\n",
    "num_epochs = 10  # max. number of epochs\n",
    "max_patience = 20  # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7APXeunMrDh"
   },
   "source": [
    "**Creating a folder for results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "bjSUn1eWLkWm"
   },
   "outputs": [],
   "source": [
    "name = 'ebm' + '_' + str(alpha) + '_' + str(sigma) + '_' + str(ld_steps)\n",
    "result_dir = 'results/' + name + '/'\n",
    "if not (os.path.exists(result_dir)):\n",
    "    os.mkdir('results')\n",
    "    os.mkdir(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hpwm6LWUMulQ"
   },
   "source": [
    "**Initializing the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrnNsCqQLmK3",
    "outputId": "5f0cf2b1-0a96-4f5c-da9e-f78f909a5259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EBM by JT.\n"
     ]
    }
   ],
   "source": [
    "energy_net = nn.Sequential(nn.Linear(D, M), nn.ELU(),\n",
    "                               nn.Linear(M, M), nn.ELU(),\n",
    "                               nn.Linear(M, M), nn.ELU(),\n",
    "                               nn.Linear(M, K))\n",
    "\n",
    "# We initialize the full model\n",
    "model = EBM(energy_net, alpha=alpha, sigma=sigma, ld_steps=ld_steps, D=D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SzTemY3NSxO"
   },
   "source": [
    "**Optimizer - here we use Adamax**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "R9TZtLVtLoWc"
   },
   "outputs": [],
   "source": [
    "# OPTIMIZER\n",
    "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNf__W_ONVHA"
   },
   "source": [
    "**Training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KhqHgluGLqIC",
    "outputId": "c52fa1e4-3376-4bff-9f87-6f03613c4e42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, val nll=0.6068514055675931, val ce=8.88888888888889, val gen_nll=-0.11144273728132248\n",
      "saved!\n",
      "Epoch: 1, val nll=0.6397722760836283, val ce=8.88888888888889, val gen_nll=-0.05703681707382202\n",
      "Epoch: 2, val nll=0.6006331510014005, val ce=8.5, val gen_nll=-0.08177360147237778\n",
      "saved!\n",
      "Epoch: 3, val nll=0.6148878071043227, val ce=8.25, val gen_nll=-0.03525439649820328\n",
      "Epoch: 4, val nll=0.624651829401652, val ce=7.75, val gen_nll=-0.009006117470562458\n",
      "Epoch: 5, val nll=0.6023856169647641, val ce=8.305555555555555, val gen_nll=-0.060453373938798904\n",
      "Epoch: 6, val nll=0.5953919755087959, val ce=7.930555555555555, val gen_nll=-0.07083692401647568\n",
      "saved!\n",
      "Epoch: 7, val nll=0.5628017021550072, val ce=8.180555555555555, val gen_nll=-0.06259763240814209\n",
      "saved!\n",
      "Epoch: 8, val nll=0.5571269690990448, val ce=8.152777777777779, val gen_nll=-0.06734920293092728\n",
      "saved!\n",
      "Epoch: 9, val nll=0.5925773315959506, val ce=8.097222222222221, val gen_nll=-0.08530569076538086\n"
     ]
    }
   ],
   "source": [
    "# Training procedure\n",
    "nll_val, error_val, gen_val = training(name=result_dir + name, max_patience=max_patience, num_epochs=num_epochs,\n",
    "                                       model=model, optimizer=optimizer,\n",
    "                                       training_loader=training_loader, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3XTxgEcNXfp"
   },
   "source": [
    "**The final evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "okK1mV_-LrRU",
    "outputId": "4664693f-742d-4453-94cf-d051d2efa9be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL PERFORMANCE: nll=0.7264786760012308, ce=8.51388888888889, gen_nll=0.03238631412386894\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_error, test_gen = evaluation(name=result_dir + name, test_loader=test_loader)\n",
    "f = open(result_dir + name + '_test_loss.txt', \"w\")\n",
    "f.write('NLL: ' + str(test_loss) + '\\nCA: ' + str(test_error) + '\\nGEN NLL: ' + str(test_gen))\n",
    "f.close()\n",
    "\n",
    "#samples_real(result_dir + name, test_loader)\n",
    "#samples_generated(result_dir + name, test_loader)\n",
    "\n",
    "plot_curve(result_dir + name, nll_val)\n",
    "plot_curve(result_dir + name, error_val, file_name='_ca_val_curve.pdf', color='r-')\n",
    "plot_curve(result_dir + name, gen_val, file_name='_gen_val_curve.pdf', color='g-')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6978260869565217\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(test_data.X).float()\n",
    "\n",
    "\n",
    "model_best = torch.load(result_dir + name + '.model')\n",
    "\n",
    "model_best.eval()\n",
    "loss = 0.\n",
    "loss_error = 0.\n",
    "loss_gen = 0.\n",
    "N = 0.\n",
    "\n",
    "y_pred = model_best.classify(X) #\n",
    "out = roc_auc_score(test_data.y, y_pred)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "vae_priors.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
